{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural language processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMN2kK3za4KMHHVelCBxAt0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desireechen/Deep-Learning/blob/master/Natural_language_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHOOg4IaJQZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H202UoM8Sdwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "16fbc3be-7803-44d9-a9ce-51c838414498"
      },
      "source": [
        "# Alternative 1. Dset that is in csv format.\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv \\\n",
        "    -O /tmp/bbc-text.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-22 05:28:55--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.203.128, 2607:f8b0:400c:c12::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.203.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5057493 (4.8M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/bbc-text.csv’\n",
            "\n",
            "\r/tmp/bbc-text.csv     0%[                    ]       0  --.-KB/s               \r/tmp/bbc-text.csv   100%[===================>]   4.82M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-06-22 05:28:55 (143 MB/s) - ‘/tmp/bbc-text.csv’ saved [5057493/5057493]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcGPuWkZImb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 2. Dset that is in json format. Get the entire dset. Then, separate training and validation.\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n",
        "    -O /tmp/sarcasm.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQL0vNJpq4Xa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 3. Dset is in Tensorflow Datasets. Dset is separated into training and validation. Similar to Compilation 3 Alternative 2A.\n",
        "import tensorflow_datasets as tfds\n",
        "imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzcoCtiXS96F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 4.\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\\n",
        "    -O /tmp/training_cleaned.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeMDQgHjHLyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 5. Two options below.\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt \\\n",
        "    -O /tmp/irish-lyrics-eof.txt\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
        "    -O /tmp/sonnets.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvkP0_ZHRQWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the 100-dimension version of GloVe from Stanford. This was used in Alternative 4.\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\\n",
        "    -O /tmp/glove.6B.100d.txt\n",
        "embeddings_index = {};\n",
        "with open('/tmp/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split();\n",
        "        word = values[0];\n",
        "        coefs = np.asarray(values[1:], dtype='float32');\n",
        "        embeddings_index[word] = coefs;\n",
        "\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word);\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDopRZp7S4J-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1.\n",
        "import csv\n",
        "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
        "sentences = []\n",
        "labels = []\n",
        "with open('/tmp/bbc-text.csv', 'r') as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter=',')\n",
        "  next(reader)\n",
        "  for row in reader:\n",
        "    labels.append(row[0])\n",
        "    sentence = row[1]\n",
        "    for word in stopwords:\n",
        "      token = ' ' + word + ' '\n",
        "      sentence = sentence.replace(token, ' ') # Replacing stopwords.\n",
        "    sentences.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HywYYQXbIqcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 2.\n",
        "import json\n",
        "with open('/tmp/sarcasm.json', 'r') as f:\n",
        "  datastore = json.load(f)\n",
        "sentences = []\n",
        "labels = []\n",
        "urls = []\n",
        "for item in datastore:\n",
        "  sentences.append(item['headline'])\n",
        "  labels.append(item['is_sarcastic'])\n",
        "  urls.append(item['article_link'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dgpd_0ZrMjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 3 is full IMDB. Alternative 3A is GRU, LSTM, Conv1D. Alternative 3B is IMDB subwords8K dataset.\n",
        "import numpy as np\n",
        "train_data, test_data = imdb['train'], imdb['test']\n",
        "train_sentences = []\n",
        "train_labels = []\n",
        "test_sentences = []\n",
        "test_labels =[]\n",
        "for s,l in train_data:\n",
        "  train_sentences.append(str(s.numpy())) # Depending on Python version, may need to use s.tonumpy instead of s.numpy\n",
        "  train_labels.append(l.numpy())\n",
        "for s,l in test_data:\n",
        "  test_sentences.append(str(s.numpy())) # Depending on Python version, may need to use s.tonumpy instead of s.numpy\n",
        "  test_labels.append(l.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz56oonLTC6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 4. \n",
        "import csv\n",
        "num_sentences = 0\n",
        "corpus =[]\n",
        "\n",
        "with open(\"/tmp/training_cleaned.csv\") as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "      # Your Code here. Create list items where the first item is the text, found in row[5], and the second is the label. Note that the label is a '0' or a '4' in the text. When it's the former, make\n",
        "      # your label to be 0, otherwise 1. Keep a count of the number of sentences in num_sentences\n",
        "        list_item=[]\n",
        "        list_item.append(row[5])\n",
        "        label = row[0]\n",
        "        if label == '4':\n",
        "          list_item.append(1)\n",
        "        else:\n",
        "          list_item.append(0)\n",
        "        num_sentences = num_sentences + 1\n",
        "        corpus.append(list_item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyxXXYb1HP98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 5.\n",
        "data = open('/tmp/irish-lyrics-eof.txt').read() # Input the file name accordingly.\n",
        "corpus = data.lower().split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrgcZcQVBoZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatives 1, 2, 3 and 3A. For Alternative 3b, only use embedding_dim.\n",
        "train_portion = 0.8\n",
        "vocab_size = 10000 # This can be 1000 or 10000.\n",
        "oov_tok = '<OOV>'\n",
        "padding_type = 'post' # This can be 'post' or 'pre'.\n",
        "trunc_type = 'post' # This can be 'post' or 'pre'.\n",
        "max_length = 120 # This can be 16, 100 or 120.\n",
        "embedding_dim = 16 # embedding_dim is used for NLP. This can be 16, 32 or 100."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxqOCBaiTonI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 4.\n",
        "train_portion = 0.9\n",
        "# vocab_size = 10000 # This can be 1000 or 10000.\n",
        "oov_tok = '<OOV>'\n",
        "padding_type = 'post' # This can be 'post' or 'pre'.\n",
        "trunc_type = 'post' # This can be 'post' or 'pre'.\n",
        "max_length = 16 # This can be 16, 100 or 120.\n",
        "embedding_dim = 100 # embedding_dim is used for NLP. This can be 16, 32 or 100.\n",
        "\n",
        "subset_corpus_size = 160000 # I can choose to take all 160000 sentences or take a subset. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z8KOUxsHUf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 5.\n",
        "# train_portion = 0.8\n",
        "# vocab_size = 10000 # This can be 1000 or 10000.\n",
        "# oov_tok = '<OOV>'\n",
        "padding_type = 'pre' # This can be 'post' or 'pre'.\n",
        "trunc_type = 'post' # This can be 'post' or 'pre'.\n",
        "# max_length = 120 # This can be 16, 100 or 120.\n",
        "embedding_dim = 100 # embedding_dim is used for NLP. This can be 16, 32 or 100."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbgPiA5_TjFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 4.\n",
        "import random\n",
        "\n",
        "sentences = []\n",
        "labels = []\n",
        "random.shuffle(corpus)\n",
        "\n",
        "for x in range(subset_corpus_size):\n",
        "  sentences.append(corpus[x][0]) #x-th row and 1st column of corpus that has the sentence\n",
        "  labels.append(corpus[x][1]) #x-th row and 2nd column of corpus that has the label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw3u5-zXbKRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1 and 4\n",
        "train_size = int(train_portion * len(labels))\n",
        "train_sentences = sentences[0:train_size]\n",
        "test_sentences = sentences[train_size:]\n",
        "train_labels = labels[0:train_size]\n",
        "test_labels = labels[train_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzBEmVCBDJET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 2.\n",
        "train_size = 20000\n",
        "train_sentences = sentences[0:train_size]\n",
        "test_sentences = sentences[train_size:]\n",
        "train_labels = labels[0:train_size]\n",
        "test_labels = labels[train_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT0-dHRiv7yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok) # May not have num_words argument.\n",
        "                                                    # Tokenizer does not care about punctuation and lower or upper case.\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_sequences, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding=padding_type, truncating=trunc_type, maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHhxbwpch1nS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 4.\n",
        "tokenizer = Tokenizer() # Just initiate Tokenizer. There are no arguments.                                 \n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) # Unique to Alternative 4.\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_sequences, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding=padding_type, truncating=trunc_type, maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOKx5Z3bHi_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 5.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1 # Plus 1 for out-of-vocabulary words.\n",
        "\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "  sequences = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(sequences)):\n",
        "    n_gram_sequence = sequences[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max([len(x) for x in input_sequences])\n",
        "input_padded = pad_sequences(input_sequences, padding=padding_type, maxlen=max_sequence_length)\n",
        "\n",
        "import numpy as np\n",
        "input_padded = np.array(input_padded) # For model training purposes, I need numpy arrays.\n",
        "predictors, labels = input_padded[:,:-1], input_padded[:,-1]\n",
        "labels = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjVnbVkiHl2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not for Alternative 1.\n",
        "import numpy as np\n",
        "train_labels = np.array(train_labels) # For model training purposes, I need numpy arrays.\n",
        "test_labels = np.array(test_labels) # For model training purposes, I need numpy arrays.\n",
        "train_padded = np.array(train_padded) # For model training purposes, I need numpy arrays.\n",
        "test_padded = np.array(test_padded) # For model training purposes, I need numpy arrays."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_79jEMP3RScy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "17149504-392d-44a0-b32f-c24b1465fbb3"
      },
      "source": [
        "# Extra code.\n",
        "print(len(sentences))\n",
        "print(len(labels))\n",
        "print(sentences[2]) # Print the 3rd sentence.\n",
        "print(padded[2]) # Print the padded sequences of the 3rd sentence.\n",
        "print(padded.shape) # 2nd item shows how many tokens (or words) that the longest sentence has."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2225\n",
            "tigers wary farrell  gamble  leicester say will not rushed making bid andy farrell great britain rugby league captain decide switch codes.   anybody else involved process still way away going next stage   tigers boss john wells told bbc radio leicester.  moment  still lot unknowns andy farrell  not least medical situation.  whoever take going take big  big gamble.  farrell  persistent knee problems  operation knee five weeks ago expected another three months. leicester saracens believed head list rugby union clubs interested signing farrell decides move 15-man game.  move across union  wells believes better off playing backs  least initially.  m sure make step league union involved centre   said wells.  think england prefer progress position back row can make use rugby league skills within forwards.  jury whether can cross divide.  club  balance will struck cost gamble option bringing ready-made replacement.\n",
            "[5000 6977 3854 ...    0    0    0]\n",
            "(2225, 2441)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hPH0xcCgKcd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a41598ca-4b45-4301-fcb0-461f2f851e30"
      },
      "source": [
        "# Alternative 1. Need to tokenize the labels which are in text. \n",
        "print(len(word_index))\n",
        "print('\\nWord Index = ', word_index)\n",
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "label_word_index = label_tokenizer.word_index\n",
        "print('\\nLabel Word Index = ', label_word_index)\n",
        "train_label_sequences = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
        "test_label_sequences = np.array(label_tokenizer.texts_to_sequences(test_labels))\n",
        "# print(train_label_sequences)\n",
        "# print(len(train_label_sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Label Word Index =  {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
            "[[4], [2], [1], [1], [5], [3], [3], [1], [1], [5], [5], [2], [2], [3], [1], [2], [3], [1], [2], [4], [4], [4], [1], [1], [4], [1], [5], [4], [3], [5], [3], [4], [5], [5], [2], [3], [4], [5], [3], [2], [3], [1], [2], [1], [4], [5], [3], [3], [3], [2], [1], [3], [2], [2], [1], [3], [2], [1], [1], [2], [2], [1], [2], [1], [2], [4], [2], [5], [4], [2], [3], [2], [3], [1], [2], [4], [2], [1], [1], [2], [2], [1], [3], [2], [5], [3], [3], [2], [5], [2], [1], [1], [3], [1], [3], [1], [2], [1], [2], [5], [5], [1], [2], [3], [3], [4], [1], [5], [1], [4], [2], [5], [1], [5], [1], [5], [5], [3], [1], [1], [5], [3], [2], [4], [2], [2], [4], [1], [3], [1], [4], [5], [1], [2], [2], [4], [5], [4], [1], [2], [2], [2], [4], [1], [4], [2], [1], [5], [1], [4], [1], [4], [3], [2], [4], [5], [1], [2], [3], [2], [5], [3], [3], [5], [3], [2], [5], [3], [3], [5], [3], [1], [2], [3], [3], [2], [5], [1], [2], [2], [1], [4], [1], [4], [4], [1], [2], [1], [3], [5], [3], [2], [3], [2], [4], [3], [5], [3], [4], [2], [1], [2], [1], [4], [5], [2], [3], [3], [5], [1], [5], [3], [1], [5], [1], [1], [5], [1], [3], [3], [5], [4], [1], [3], [2], [5], [4], [1], [4], [1], [5], [3], [1], [5], [4], [2], [4], [2], [2], [4], [2], [1], [2], [1], [2], [1], [5], [2], [2], [5], [1], [1], [3], [4], [3], [3], [3], [4], [1], [4], [3], [2], [4], [5], [4], [1], [1], [2], [2], [3], [2], [4], [1], [5], [1], [3], [4], [5], [2], [1], [5], [1], [4], [3], [4], [2], [2], [3], [3], [1], [2], [4], [5], [3], [4], [2], [5], [1], [5], [1], [5], [3], [2], [1], [2], [1], [1], [5], [1], [3], [3], [2], [5], [4], [2], [1], [2], [5], [2], [2], [2], [3], [2], [3], [5], [5], [2], [1], [2], [3], [2], [4], [5], [2], [1], [1], [5], [2], [2], [3], [4], [5], [4], [3], [2], [1], [3], [2], [5], [4], [5], [4], [3], [1], [5], [2], [3], [2], [2], [3], [1], [4], [2], [2], [5], [5], [4], [1], [2], [5], [4], [4], [5], [5], [5], [3], [1], [3], [4], [2], [5], [3], [2], [5], [3], [3], [1], [1], [2], [3], [5], [2], [1], [2], [2], [1], [2], [3], [3], [3], [1], [4], [4], [2], [4], [1], [5], [2], [3], [2], [5], [2], [3], [5], [3], [2], [4], [2], [1], [1], [2], [1], [1], [5], [1], [1], [1], [4], [2], [2], [2], [3], [1], [1], [2], [4], [2], [3], [1], [3], [4], [2], [1], [5], [2], [3], [4], [2], [1], [2], [3], [2], [2], [1], [5], [4], [3], [4], [2], [1], [2], [5], [4], [4], [2], [1], [1], [5], [3], [3], [3], [1], [3], [4], [4], [5], [3], [4], [5], [2], [1], [1], [4], [2], [1], [1], [3], [1], [1], [2], [1], [5], [4], [3], [1], [3], [4], [2], [2], [2], [4], [2], [2], [1], [1], [1], [1], [2], [4], [5], [1], [1], [4], [2], [4], [5], [3], [1], [2], [3], [2], [4], [4], [3], [4], [2], [1], [2], [5], [1], [3], [5], [1], [1], [3], [4], [5], [4], [1], [3], [2], [5], [3], [2], [5], [1], [1], [4], [3], [5], [3], [5], [3], [4], [3], [5], [1], [2], [1], [5], [1], [5], [4], [2], [1], [3], [5], [3], [5], [5], [5], [3], [5], [4], [3], [4], [4], [1], [1], [4], [4], [1], [5], [5], [1], [4], [5], [1], [1], [4], [2], [3], [4], [2], [1], [5], [1], [5], [3], [4], [5], [5], [2], [5], [5], [1], [4], [4], [3], [1], [4], [1], [3], [3], [5], [4], [2], [4], [4], [4], [2], [3], [3], [1], [4], [2], [2], [5], [5], [1], [4], [2], [4], [5], [1], [4], [3], [4], [3], [2], [3], [3], [2], [1], [4], [1], [4], [3], [5], [4], [1], [5], [4], [1], [3], [5], [1], [4], [1], [1], [3], [5], [2], [3], [5], [2], [2], [4], [2], [5], [4], [1], [4], [3], [4], [3], [2], [3], [5], [1], [2], [2], [2], [5], [1], [2], [5], [5], [1], [5], [3], [3], [3], [1], [1], [1], [4], [3], [1], [3], [3], [4], [3], [1], [2], [5], [1], [2], [2], [4], [2], [5], [5], [5], [2], [5], [5], [3], [4], [2], [1], [4], [1], [1], [3], [2], [1], [4], [2], [1], [4], [1], [1], [5], [1], [2], [1], [2], [4], [3], [4], [2], [1], [1], [2], [2], [2], [2], [3], [1], [2], [4], [2], [1], [3], [2], [4], [2], [1], [2], [3], [5], [1], [2], [3], [2], [5], [2], [2], [2], [1], [3], [5], [1], [3], [1], [3], [3], [2], [2], [1], [4], [5], [1], [5], [2], [2], [2], [4], [1], [4], [3], [4], [4], [4], [1], [4], [4], [5], [5], [4], [1], [5], [4], [1], [1], [2], [5], [4], [2], [1], [2], [3], [2], [5], [4], [2], [3], [2], [4], [1], [2], [5], [2], [3], [1], [5], [3], [1], [2], [1], [3], [3], [1], [5], [5], [2], [2], [1], [4], [4], [1], [5], [4], [4], [2], [1], [5], [4], [1], [1], [2], [5], [2], [2], [2], [5], [1], [5], [4], [4], [4], [3], [4], [4], [5], [5], [1], [1], [3], [2], [5], [1], [3], [5], [4], [3], [4], [4], [2], [5], [3], [4], [3], [3], [1], [3], [3], [5], [4], [1], [3], [1], [5], [3], [2], [2], [3], [1], [1], [1], [5], [4], [4], [2], [5], [1], [3], [4], [3], [5], [4], [4], [2], [2], [1], [2], [2], [4], [3], [5], [2], [2], [2], [2], [2], [4], [1], [3], [4], [4], [2], [2], [5], [3], [5], [1], [4], [1], [5], [1], [4], [1], [2], [1], [3], [3], [5], [2], [1], [3], [3], [1], [5], [3], [2], [4], [1], [2], [2], [2], [5], [5], [4], [4], [2], [2], [5], [1], [2], [5], [4], [4], [2], [2], [1], [1], [1], [3], [3], [1], [3], [1], [2], [5], [1], [4], [5], [1], [1], [2], [2], [4], [4], [1], [5], [1], [5], [1], [5], [3], [5], [5], [4], [5], [2], [2], [3], [1], [3], [4], [2], [3], [1], [3], [1], [5], [1], [3], [1], [1], [4], [5], [1], [3], [1], [1], [2], [4], [5], [3], [4], [5], [3], [5], [3], [5], [5], [4], [5], [3], [5], [5], [4], [4], [1], [1], [5], [5], [4], [5], [3], [4], [5], [2], [4], [1], [2], [5], [5], [4], [5], [4], [2], [5], [1], [5], [2], [1], [2], [1], [3], [4], [5], [3], [2], [5], [5], [3], [2], [5], [1], [3], [1], [2], [2], [2], [2], [2], [5], [4], [1], [5], [5], [2], [1], [4], [4], [5], [1], [2], [3], [2], [3], [2], [2], [5], [3], [2], [2], [4], [3], [1], [4], [5], [3], [2], [2], [1], [5], [3], [4], [2], [2], [3], [2], [1], [5], [1], [5], [4], [3], [2], [2], [4], [2], [2], [1], [2], [4], [5], [3], [2], [3], [2], [1], [4], [2], [3], [5], [4], [2], [5], [1], [3], [3], [1], [3], [2], [4], [5], [1], [1], [4], [2], [1], [5], [4], [1], [3], [1], [2], [2], [2], [3], [5], [1], [3], [4], [2], [2], [4], [5], [5], [4], [4], [1], [1], [5], [4], [5], [1], [3], [4], [2], [1], [5], [2], [2], [5], [1], [2], [1], [4], [3], [3], [4], [5], [3], [5], [2], [2], [3], [1], [4], [1], [1], [1], [3], [2], [1], [2], [4], [1], [2], [2], [1], [3], [4], [1], [2], [4], [1], [1], [2], [2], [2], [2], [3], [5], [4], [2], [2], [1], [2], [5], [2], [5], [1], [3], [2], [2], [4], [5], [2], [2], [2], [3], [2], [3], [4], [5], [3], [5], [1], [4], [3], [2], [4], [1], [2], [2], [5], [4], [2], [2], [1], [1], [5], [1], [3], [1], [2], [1], [2], [3], [3], [2], [3], [4], [5], [1], [2], [5], [1], [3], [3], [4], [5], [2], [3], [3], [1], [4], [2], [1], [5], [1], [5], [1], [2], [1], [3], [5], [4], [2], [1], [3], [4], [1], [5], [2], [1], [5], [1], [4], [1], [4], [3], [1], [2], [5], [4], [4], [3], [4], [5], [4], [1], [2], [4], [2], [5], [1], [4], [3], [3], [3], [3], [5], [5], [5], [2], [3], [3], [1], [1], [4], [1], [3], [2], [2], [4], [1], [4], [2], [4], [3], [3], [1], [2], [3], [1], [2], [4], [2], [2], [5], [5], [1], [2], [4], [4], [3], [2], [3], [1], [5], [5], [3], [3], [2], [2], [4], [4], [1], [1], [3], [4], [1], [4], [2], [1], [2], [3], [1], [5], [2], [4], [3], [5], [4], [2], [1], [5], [4], [4], [5], [3], [4], [5], [1], [5], [1], [1], [1], [3], [4], [1], [2], [1], [1], [2], [4], [1], [2], [5], [3], [4], [1], [3], [4], [5], [3], [1], [3], [4], [2], [5], [1], [3], [2], [4], [4], [4], [3], [2], [1], [3], [5], [4], [5], [1], [4], [2], [3], [5], [4], [3], [1], [1], [2], [5], [2], [2], [3], [2], [2], [3], [4], [5], [3], [5], [5], [2], [3], [1], [3], [5], [1], [5], [3], [5], [5], [5], [2], [1], [3], [1], [5], [4], [4], [2], [3], [5], [2], [1], [2], [3], [3], [2], [1], [4], [4], [4], [2], [3], [3], [2], [1], [1], [5], [2], [1], [1], [3], [3], [3], [5], [3], [2], [4], [2], [3], [5], [5], [2], [1], [3], [5], [1], [5], [3], [3], [2], [3], [1], [5], [5], [4], [4], [4], [4], [3], [4], [2], [4], [1], [1], [5], [2], [4], [5], [2], [4], [1], [4], [5], [5], [3], [3], [1], [2], [2], [4], [5], [1], [3], [2], [4], [5], [3], [1], [5], [3], [3], [4], [1], [3], [2], [3], [5], [4], [1], [3], [5], [5], [2], [1], [4], [4], [1], [5], [4], [3], [4], [1], [3], [3], [1], [5], [1], [3], [1], [4], [5], [1], [5], [2], [2], [5], [5], [5], [4], [1], [2], [2], [3], [3], [2], [3], [5], [1], [1], [4], [3], [1], [2], [1], [2], [4], [1], [1], [2], [5], [1], [1], [4], [1], [2], [3], [2], [5], [4], [5], [3], [2], [5], [3], [5], [3], [3], [2], [1], [1], [1], [4], [4], [1], [3], [5], [4], [1], [5], [2], [5], [3], [2], [1], [4], [2], [1], [3], [2], [5], [5], [5], [3], [5], [3], [5], [1], [5], [1], [3], [3], [2], [3], [4], [1], [4], [1], [2], [3], [4], [5], [5], [3], [5], [3], [1], [1], [3], [2], [4], [1], [3], [3], [5], [1], [3], [3], [2], [4], [4], [2], [4], [1], [1], [2], [3], [2], [4], [1], [4], [3], [5], [1], [2], [1], [5], [4], [4], [1], [3], [1], [2], [1], [2], [1], [1], [5], [5], [2], [4], [4], [2], [4], [2], [2], [1], [1], [3], [1], [4], [1], [4], [1], [1], [2], [2], [4], [1], [2], [4], [4], [3], [1], [2], [5], [5], [4], [3], [1], [1], [4], [2], [4], [5], [5], [3], [3], [2], [5], [1], [5], [5], [2], [1], [3], [4], [2], [1], [5], [4], [3], [3], [1], [1], [2], [2], [2], [2], [2], [5], [2], [3], [3], [4], [4], [5], [3], [5], [2], [3], [1], [1], [2], [4], [2], [4], [1], [2], [2], [3], [1], [1], [3], [3], [5], [5], [3], [2], [3], [3], [2], [4], [3], [3], [3], [3], [3], [5], [5], [4], [3], [1], [3], [1], [4], [1], [1], [1], [5], [4], [5], [4], [1], [4], [1], [1], [5], [5], [2], [5], [5], [3], [2], [1], [4], [4], [3], [2], [1], [2], [5], [1], [3], [5], [1], [1], [2], [3], [4], [4], [2], [2], [1], [3], [5], [1], [1], [3], [5], [4], [1], [5], [2], [3], [1], [3], [4], [5], [1], [3], [2], [5], [3], [5], [3], [1], [3], [2], [2], [3], [2], [4], [1], [2], [5], [2], [1], [1], [5], [4], [3], [4], [3], [3], [1], [1], [1], [2], [4], [5], [2], [1], [2], [1], [2], [4], [2], [2], [2], [2], [1], [1], [1], [2], [2], [5], [2], [2], [2], [1], [1], [1], [4], [2], [1], [1], [1], [2], [5], [4], [4], [4], [3], [2], [2], [4], [2], [4], [1], [1], [3], [3], [3], [1], [1], [3], [3], [4], [2], [1], [1], [1], [1], [2], [1], [2], [2], [2], [2], [1], [3], [1], [4], [4], [1], [4], [2], [5], [2], [1], [2], [4], [4], [3], [5], [2], [5], [2], [4], [3], [5], [3], [5], [5], [4], [2], [4], [4], [2], [3], [1], [5], [2], [3], [5], [2], [4], [1], [4], [3], [1], [3], [2], [3], [3], [2], [2], [2], [4], [3], [2], [3], [2], [5], [3], [1], [3], [3], [1], [5], [4], [4], [2], [4], [1], [2], [2], [3], [1], [4], [4], [4], [1], [5], [1], [3], [2], [3], [3], [5], [4], [2], [4], [1], [5], [5], [1], [2], [5], [4], [4], [1], [5], [2], [3], [3], [3], [4], [4], [2], [3], [2], [3], [3], [5], [1], [4], [2], [4], [5], [4], [4], [1], [3], [1], [1], [3], [5], [5], [2], [3], [3], [1], [2], [2], [4], [2], [4], [4], [1], [2], [3], [1], [2], [2], [1], [4], [1], [4], [5], [1], [1], [5], [2], [4], [1], [1], [3], [4], [2], [3], [1], [1], [3], [5], [4], [4], [4], [2], [1], [5], [5], [4], [2], [3], [4], [1], [1], [4], [4], [3], [2], [1], [5], [5], [1], [5], [4], [4], [2], [2], [2], [1], [1], [4], [1], [2], [4], [2], [2], [1], [2], [3], [2], [2], [4], [2], [4], [3], [4], [5], [3], [4], [5], [1], [3], [5], [2], [4], [2], [4], [5], [4], [1], [2], [2], [3], [5], [3], [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpDaKHergAom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1 where there are 6 categories.\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "                             # tf.keras.layers.Dropout(0.5), # This is the OPTIONAL dropout layer. This number can be 0.2 or 0.5 etc.\n",
        "                             # Dropouts remove a random number of neurons in your neural network. Dropout is a regularization technique.\n",
        "                             ## CHOOSE either one of the next two lines.\n",
        "                             # tf.keras.layers.GlobalAveragePooling1D(), # Averages across the vector to flatten it out.\n",
        "                             # tf.keras.layers.Flatten(),\n",
        "                             tf.keras.layers.Dense(24, activation='relu'), # This number can be 6 or 24.\n",
        "                             tf.keras.layers.Dense(6, activation='sigmoid')])\n",
        "# model.summary()\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # optimizer=tf.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r1cnqKtzKWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatives 2, 3 and 3A.\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "                             # tf.keras.layers.Dropout(0.5), # This is the OPTIONAL dropout layer. This number can be 0.2 or 0.5 etc.\n",
        "                             # Dropouts remove a random number of neurons in your neural network. Dropout is a regularization technique.\n",
        "                             ### tf.keras.layers.Conv1D(128, 5, activation='relu'), # 5 is the size of the filter. Number can be 64 or 128.\n",
        "                             # CHOOSE either one of the next five options.\n",
        "                             ### tf.keras.layers.GlobalAveragePooling1D(), # Averages across the vector to flatten it out.\n",
        "                             ### tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "                             # tf.keras.layers.Flatten(),\n",
        "                             # tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)), # This is for GRU.\n",
        "                             # tf.keras.layers.LSTM(64), # This number can be 32, 64, 100 or 150.\n",
        "                             # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # This is for LSTM. # This number can be 32, 64, 100 or 150.\n",
        "                             tf.keras.layers.Dense(6, activation='relu'), # This number can be 6, 24 or 64.\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "# model.summary()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # optimizer=tf.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90_KyPq5xbft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 3B. Single-layer and Multi-layer LSTM. Also can do Conv1D with GlobalAveragePooling1D.\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
        "                             # tf.keras.layers.Dropout(0.5), # This is the OPTIONAL dropout layer. This number can be 0.2 or 0.5 etc.\n",
        "                             # Dropouts remove a random number of neurons in your neural network. Dropout is a regularization technique.\n",
        "                             ### tf.keras.layers.Conv1D(128, 5, activation='relu'), # 5 is the size of the filter. Number can be 64 or 128.\n",
        "                             # CHOOSE either one of the next five options.\n",
        "                             ### tf.keras.layers.GlobalAveragePooling1D(), # Averages across the vector to flatten it out.\n",
        "                             ### tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "                             # tf.keras.layers.Flatten(),\n",
        "                             # tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)), # This is for GRU.\n",
        "                             # tf.keras.layers.LSTM(64),\n",
        "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)), # This number can be 32, 64, 100 or 150.\n",
        "                             # Need return_sequences for multi-layer LSTM.\n",
        "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # Single-layer LSTM. # This number can be 32, 64, 100 or 150.\n",
        "                             tf.keras.layers.Dense(64, activation='relu'), # This number can be 6, 24 or 64.\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "# model.summary()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # optimizer=tf.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gICFC08QUfyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 4.\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length,\n",
        "                                                       weights=[embeddings_matrix], trainable=False),\n",
        "                             tf.keras.layers.Dropout(0.2), # This is the OPTIONAL dropout layer. This number can be 0.2 or 0.5 etc.\n",
        "                             # Dropouts remove a random number of neurons in your neural network. Dropout is a regularization technique.\n",
        "                             tf.keras.layers.Conv1D(64, 5, activation='relu'), # 5 is the size of the filter.\n",
        "                             # CHOOSE either one of the next five options.\n",
        "                             ### tf.keras.layers.GlobalAveragePooling1D(), # Averages across the vector to flatten it out.\n",
        "                             tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "                             # tf.keras.layers.Flatten(),\n",
        "                             # tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)), # This is for GRU.\n",
        "                             # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # This is for LSTM. # This number can be 32, 64, 100 or 150.\n",
        "                             tf.keras.layers.LSTM(64), # This number can be 32, 64, 100 or 150.\n",
        "                             # tf.keras.layers.Dense(6, activation='relu'), # This number can be 6, 24 or 64.\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "# model.summary()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # optimizer=tf.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O_LzlPjH0vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 5.\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import regularizers\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length= max_sequence_length-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Dropout(0.2)) # This is the OPTIONAL dropout layer. This number can be 0.2 or 0.5 etc.\n",
        "                        # Dropouts remove a random number of neurons in your neural network. Dropout is a regularization technique.\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(vocab_size/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "# model.summary()\n",
        "\n",
        "# EITHER do not wish to set learning rate of the Adam optimiser.\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # optimizer=tf.optimizers.Adam()\n",
        "# OR wish to set learning rate of the Adam optimiser.\n",
        "optimizer = Adam(lr=0.01)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) # optimizer=tf.optimizers.Adam()\n",
        "\n",
        "# Can have an early stop\n",
        "# earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqK5Hwun-Aev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1.\n",
        "num_epochs = 10 # This number can be 10, 30, 50 or 100.\n",
        "history = model.fit(train_padded, train_label_sequences, epochs=num_epochs, validation_data=(test_padded, test_label_sequences), verbose=2)\n",
        "# verbose 0, 1, 2 shows the least, most, in between information.\n",
        "# verbose 1 or 2 will show training loss, training accuracy, validation loss and validation accuracy. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z7IKhM-0uzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatives 2, 3, 3A and 4.\n",
        "num_epochs = 50 # This number can be 10, 30, 50 or 100.\n",
        "history = model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(test_padded, test_labels), verbose=2)\n",
        "# verbose 0, 1, 2 shows the least, most, in between information.\n",
        "# verbose 1 or 2 will show training loss, training accuracy, validation loss and validation accuracy. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iG5wudMwKQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 3B.\n",
        "num_epochs = 10 # This number can be 10, 30, 50 or 100.\n",
        "history = model.fit(train_data, epochs=num_epochs, validation_data=test_data, verbose=1)\n",
        "# verbose 0, 1, 2 shows the least, most, in between information.\n",
        "# verbose 1 or 2 will show training loss, training accuracy, validation loss and validation accuracy. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kRbBlAgH6oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 5.\n",
        "num_epochs = 100 # This number can be 10, 30, 50 or 100.\n",
        "history = model.fit(predictors, labels, epochs=num_epochs, verbose=1)\n",
        "# verbose 0, 1, 2 shows the least, most, in between information.\n",
        "# verbose 1 or 2 will show training loss, training accuracy, validation loss and validation accuracy. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mLGvlPD3cVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra code.\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "def decode_sentence(text):\n",
        "  return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "print(decode_sentence(train_padded[2])) # This is the 3rd padded train sentence.\n",
        "print(train_sentences[2]) # This is the original sentence including punctuation, capital letters and OOV.\n",
        "print(train_labels[2])\n",
        "\n",
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # In the format of (vocab_size, embedding_dim) showing how many vocabulary words and dimensions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9SIyP1O4z0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra code.\n",
        "import io\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  # Write out the words.\n",
        "  out_m.write(word + \"\\n\")\n",
        "  # Write out the coefficients of each dimension on the vector for the word.\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32lGoTd75s9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra code. Test a given sentence.\n",
        "# EITHER THIS. sentences = ['I really think this is amazing. honest.']\n",
        "# OR THIS. sentences = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print('\\n Sequence(s) = ', sequences)\n",
        "print('\\nWord Index = ', word_index) # Print this for reference.\n",
        "padded = pad_sequences(sequences, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n",
        "print('\\nPadded:')\n",
        "print(model.predict(padded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLPR9pcsKqCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\") # Lower loss means less confidence in the prediction."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eBnNayTdaBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra code.\n",
        "model.save(\"test_bidirectional_LSTM.h5\")\n",
        "model.save(\"test_1Dconvolutional_layer.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8rTEzq3GPDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra code. Predict from given seed text.\n",
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\" # Can change the seed text.\n",
        "next_words = 100 # Probably can use less words. \n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre') # Pad the seed text, so that it matches the model.\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0) # Use the model to predict what comes after the seed text. \n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}