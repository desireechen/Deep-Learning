{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6iAruQ5158gscAfvvnoxl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desireechen/Deep-Learning/blob/master/Image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJVqMRYB17c3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1. Get the entire dset. No separation of training and validation. Everything is used for training.\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/a-or-b.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HhbREru2GxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1A. Get the training and validation dset separately.\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/a-or-b.zip\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
        "    -O /tmp/validation-a-or-b.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IipIYyqQY0Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 2. Get the entire dset. Then, separate training and validation.\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n",
        "    -O \"/tmp/cats-and-dogs.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErOV3Zu3k252",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 2A. Get the entire dset which already has the data separated into training and validation.\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeZqiaJmo-HE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 3. Multi-class classification.\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip \\\n",
        "    -O /tmp/rps.zip\n",
        "  \n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip \\\n",
        "    -O /tmp/rps-test-set.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40rnmYxoPXZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVymX1yrUOUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transfer Learning.\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 # Snapshot of the model after being trained.\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5' \n",
        "pre_trained_model = InceptionV3(input_shape = (150, 150, 3), # This is the desired input shape of the data.\n",
        "                                include_top = False, # Ignore the fully-connected layer at the top of InceptionV3.\n",
        "                                weights = None) # I do not wish to use the built-in weights. I wish to use the snapshot that was downloaded. \n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False # Lock the pre-trained layers of the model.\n",
        "# pre_trained_model.summary()\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('Last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbSIoXcVI5hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1.\n",
        "local_zip = '/tmp/a-or-b.zip' # Insert name of zip file.\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/a-or-b') \n",
        "zip_ref.close()\n",
        "\n",
        "# Define each of the directories.\n",
        "train_a_dir = os.path.join('/tmp/a-or-b/a') # Fill in the correct path. If the file is horses, then replace a with horses.\n",
        "train_b_dir = os.path.join('/tmp/a-or-b/b') # Fill in the correct path. If the file is humans, then replace b with humans."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcMhFjfT2vLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1A and 3. For Alternative 1 above, only need half the lines. Remember to close the last line.\n",
        "local_zip = '/tmp/a-or-b.zip' # Insert name of zip file.\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/a-or-b') ## For Alternative 3, just extract to ('/tmp/')\n",
        "zip_ref.close()\n",
        "local_zip = '/tmp/validation-a-or-b.zip' # Insert name of zip file.\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/validation-a-or-b') ## For Alternative 3, just extract to ('/tmp/')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define each of the directories.\n",
        "train_a_dir = os.path.join('/tmp/a-or-b/a') # Fill in the correct path. If the file is horses, then replace a with horses.\n",
        "train_b_dir = os.path.join('/tmp/a-or-b/b') # Fill in the correct path. If the file is humans, then replace b with humans.\n",
        "rock_dir = os.path.join('/tmp/rps/rock') ## This is for Alternative 3. Do the same for paper and scissors.\n",
        "validation_a_dir = os.path.join('/tmp/validation-a-or-b/a') # Fill in the correct path. If the file is horses, then replace a with horses.\n",
        "validation_b_dir = os.path.join('/tmp/validation-a-or-b/b') # Fill in the correct path. If the file is humans, then replace b with humans."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPSEFp9SQio-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 2. Separate training and validation myself.\n",
        "local_zip = '/tmp/cats-and-dogs.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# Make each of the directories including base directory.\n",
        "try:\n",
        "  os.mkdir('/tmp/base_dir')\n",
        "  os.mkdir('/tmp/base_dir/train_dir')\n",
        "  os.mkdir('/tmp/base_dir/validation_dir')\n",
        "  os.mkdir('/tmp/base_dir/train_dir/a/')\n",
        "  os.mkdir('/tmp/base_dir/validation_dir/a/')\n",
        "  os.mkdir('/tmp/base_dir/train_dir/b/')\n",
        "  os.mkdir('/tmp/base_dir/validation_dir/b/')\n",
        "except OSError:\n",
        "  pass\n",
        "\n",
        "import random\n",
        "from shutil import copyfile\n",
        "# Function to randomise files, then split into train and validation based on a split proportion.\n",
        "def split_data(SOURCE, TRAIN, VALIDATION, SPLIT_SIZE):\n",
        "  files = []\n",
        "  for fname in os.listdir(SOURCE):\n",
        "    path = SOURCE + fname # I am making the path, so that later on I can get the size of the file that is in the path.\n",
        "    if os.path.getsize(path) > 0: # Checking if file is of zero length.\n",
        "      files.append(fname)\n",
        "    else:\n",
        "      print(fname + \" is zero length, so ignoring.\")\n",
        "\n",
        "  train_length = int(len(files) * SPLIT_SIZE)\n",
        "  validation_length = int(len(files) - train_length)\n",
        "  shuffled_set = random.sample(files, len(files))\n",
        "  train_set = shuffled_set[0:train_length]\n",
        "  validation_set = shuffled_set[-validation_length:]\n",
        "\n",
        "  for fname in train_set:\n",
        "    source = SOURCE + fname\n",
        "    destination = TRAIN + fname\n",
        "    copyfile(source, destination)\n",
        "\n",
        "  for fname in validation_set:\n",
        "    source = SOURCE + fname\n",
        "    destination = VALIDATION + fname\n",
        "    copyfile(source, destination)\n",
        "\n",
        "# Define arguments in the function.\n",
        "a_source_dir = '/tmp/PetImages/a/' # Insert path accordingly. If the file is Cat, then replace a with Cat.\n",
        "train_a_dir = '/tmp/base_dir/train_dir/a/'\n",
        "validation_a_dir = '/tmp/base_dir/validation_dir/a/'\n",
        "b_source_dir = '/tmp/PetImages/b/' # Insert path accordingly. If the file is Dog, then replace b with Dog.\n",
        "train_b_dir = '/tmp/base_dir/train_dir/b/'\n",
        "validation_b_dir = '/tmp/base_dir/validation_dir/b/'\n",
        "split_size = 0.9\n",
        "\n",
        "split_data(a_source_dir, train_a_dir, validation_a_dir, split_size)\n",
        "split_data(b_source_dir, train_b_dir, validation_b_dir, split_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZKle7r6lrBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 2A. Entire dset is separated into training and validation.\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define each of the directories.\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "train_a_dir = os.path.join(train_dir, 'a') # Insert path accordingly. If the file is Cat, then replace a with Cat.\n",
        "train_b_dir = os.path.join(train_dir, 'b') # Insert path accordingly. If the file is Dog, then replace b with Dog.\n",
        "validation_a_dir = os.path.join(validation_dir, 'a') # Insert path accordingly. If the file is Cat, then replace a with Cat.\n",
        "validation_b_dir = os.path.join(validation_dir, 'b') # Insert path accordingly. If the file is Dog, then replace b with Dog."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWRhbJXoxsYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 4. Using csv files.\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "def get_data(filename):\n",
        "    with open(filename) as training_file:\n",
        "        csv_reader = csv.reader(training_file, delimiter=',')\n",
        "        first_line = True\n",
        "        temp_images = []\n",
        "        temp_labels = []\n",
        "        for row in csv_reader:\n",
        "            if first_line:\n",
        "              first_line = False # Ignore the 1st line containing the column headers.\n",
        "            else:\n",
        "                temp_labels.append(row[0]) # This has the label of the image.\n",
        "                image_data = row[1:785] # These are the pixel values for the images.\n",
        "                image_data_as_array = np.array_split(image_data, 28) # Turn the 784 pixels into 28x28.\n",
        "                temp_images.append(image_data_as_array)\n",
        "        images = np.array(temp_images).astype('float') # Data is in strings. Need to convert to float.\n",
        "        labels = np.array(temp_labels).astype('float') # Data is in strings. Need to convert to float.\n",
        "    return images, labels\n",
        "\n",
        "training_images, training_labels = get_data('sign_mnist_train.csv')\n",
        "testing_images, testing_labels = get_data('sign_mnist_test.csv')\n",
        "\n",
        "# print(training_images.shape)\n",
        "# print(training_labels.shape)\n",
        "# print(testing_images.shape)\n",
        "# print(testing_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_-DPgOh6Y0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra code. \n",
        "# See the filenames using os.listdir\n",
        "train_a_fnames = os.listdir(train_a_dir)\n",
        "print(train_a_fnames[:10])\n",
        "train_b_names = os.listdir(train_b_dir)\n",
        "print(train_b_names[:10])\n",
        "validation_a_fnames = os.listdir(validation_a_dir)\n",
        "print(validation_a_fnames[:10])\n",
        "validation_b_names = os.listdir(validation_b_dir)\n",
        "print(validation_b_names[:10])\n",
        "\n",
        "rock_files = os.listdir(rock_dir) ## This is for Alternative 3. \n",
        "print(rock_files[:10])\n",
        "paper_files = os.listdir(paper_dir) ## This is for Alternative 3. \n",
        "print(rock_files[:10])\n",
        "scissors_files = os.listdir(scissors_dir) ## This is for Alternative 3. \n",
        "print(rock_files[:10])\n",
        "\n",
        "# See the total number of images in each of the directories.\n",
        "print('total training A images:', len(os.listdir(train_a_dir)))\n",
        "print('total training B images:', len(os.listdir(train_b_dir)))\n",
        "print('total validation A images:', len(os.listdir(validation_a_dir)))\n",
        "print('total validation B images:', len(os.listdir(validation_b_dir)))\n",
        "\n",
        "# See the total number of images in each of the paths.\n",
        "print(len(os.listdir('/tmp/PetImages/Cat/')))\n",
        "print(len(os.listdir('/tmp/PetImages/Dog/')))\n",
        "print(len(os.listdir('/tmp/base_dir/train_dir/a/')))\n",
        "print(len(os.listdir('/tmp/base_dir/train_dir/b/')))\n",
        "print(len(os.listdir('/tmp/base_dir/validation_dir/a/')))\n",
        "print(len(os.listdir('/tmp/base_dir/validation_dir/b/')))\n",
        "\n",
        "# See the size of the file. Check if the file is of zero length.\n",
        "os.path.getsize('/tmp/base_dir/train_dir/train_a_dir/2354.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_17JfoE7yfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra code. Take a look at the images. This is for binary classification.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "# Set graph to output images in 4-by-4 configuration.\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "pic_index = 0 # Index for iterating over images.\n",
        "# Set up matplotlib fig and size it to fit 4-by-4 configuration.\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols*4, nrows*4)\n",
        "\n",
        "pic_index += 8\n",
        "next_a_pix = [os.path.join(train_a_dir, fname) for fname in train_a_fnames[pic_index-8:pic_index]]\n",
        "next_b_pix = [os.path.join(train_b_dir, fname) for fname in train_b_fnames[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_a_pix+next_b_pix):\n",
        "  # Set up subplot.\n",
        "  sp = plt.subplot(nrows, ncols, i+1)\n",
        "  sp.axis('Off') # I do not wish to show gridlines.\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-zUJyLTq1LA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra code. Take a look at the images. This is for MULTI-CLASS CLASSIFICATION.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 2\n",
        "next_rock = [os.path.join(rock_dir, fname) for fname in rock_files[pic_index-2:pic_index]]\n",
        "next_paper = [os.path.join(paper_dir, fname) for fname in paper_files[pic_index-2:pic_index]]\n",
        "next_scissors = [os.path.join(scissors_dir, fname) for fname in scissors_files[pic_index-2:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_rock+next_paper+next_scissors):\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('Off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZfTKFpctD9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Preprocessing. Alternatives below.\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MRQyGAJs3fT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative AUGMENTATION. Validation data should not be augmented.\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255, # Another has it as rescale = 1./255.,\n",
        "    rotation_range = 40,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = 'nearest' # Some do not have this line.\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_DMIyh-Jdgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1. Data Preprocessing. Set up data generators that will read images from subdirectories and auto label with the subdirectory name. \n",
        "train_datagen = ImageDataGenerator(rescale=1/255) # Another has it as (rescale=1.0/255. ) or (rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/tmp/a-or-b/', # Point the generator to the directory that contains the sub-directories.\n",
        "    target_size=(300,300), # This is ensuring that images are uniformly resized. Images are resized as they are loaded. No change to the source image.\n",
        "                            # If compact, then use (150,150) or whatever.\n",
        "    batch_size=128,\n",
        "    class_mode='binary' # We need binary labels for binary classification problem. \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rmbLOwaEzA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1A. Data Preprocessing. Set up data generators that will read images from subdirectories and auto label with the subdirectory name.  \n",
        "train_datagen = ImageDataGenerator(rescale=1/255) # Another has it as (rescale=1.0/255. ) or (rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255) # Another has it as (rescale=1.0/255. ) or (rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/tmp/a-or-b/', # Point the generator to the directory that contains the sub-directories.\n",
        "    target_size=(300,300), # This is ensuring that images are uniformly resized. Images are resized as they are loaded. No change to the source image.\n",
        "                            # If compact, then use (150,150) or whatever.\n",
        "    batch_size=128,\n",
        "    class_mode='binary' # We need binary labels for binary classification problem. \n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    '/tmp/validation-a-or-b/', # Point the generator to the directory that contains the sub-directories.\n",
        "    target_size=(300,300), # This is ensuring that images are uniformly resized. Images are resized as they are loaded. No change to the source image.\n",
        "                            # If compact, then use (150,150) or whatever.\n",
        "    batch_size=32,\n",
        "    class_mode='binary' # We need binary labels for binary classification problem. \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NVUIlgUZzfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 2 and 2A. Data Preprocessing. Set up data generators that will read images from subdirectories and auto label with the subdirectory name.  \n",
        "train_datagen = ImageDataGenerator(rescale=1/255) # Another has it as (rescale=1.0/255. ) or (rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255) # Another has it as (rescale=1.0/255. ) or (rescale=1./255)\n",
        "\n",
        "train_dir = '/tmp/base_dir/train_dir/' ## This line is not required for Alternative 2A.\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, # Point the generator to the directory called Train that contains the sub-directories A or B.\n",
        "    target_size=(300,300), # This is ensuring that images are uniformly resized. Images are resized as they are loaded. No change to the source image.\n",
        "                            # If compact, then use (150,150) or whatever.\n",
        "    batch_size=20, # If there are 1000 images and batch size is 20, there will be 50 batches.\n",
        "    class_mode='binary' # We need binary labels for binary classification problem. \n",
        ")\n",
        "\n",
        "validation_dir = '/tmp/base_dir/validation_dir/' ## This line is not required for Alternative 2A.\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir, # Point the generator to the directory called Train that contains the sub-directories A or B.\n",
        "    target_size=(300,300), # This is ensuring that images are uniformly resized. Images are resized as they are loaded. No change to the source image.\n",
        "                            # If compact, then use (150,150) or whatever.\n",
        "    batch_size=20, # If there are 1000 images and batch size is 20, there will be 50 batches.\n",
        "    class_mode='binary' # We need binary labels for binary classification problem. \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeMDTphutz4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 3. Data Preprocessing. Set up data generators that will read images from subdirectories and auto label with the subdirectory name. \n",
        "train_datagen = ImageDataGenerator(rescale=1/255) # Another has it as (rescale=1.0/255. ) or (rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255) # Another has it as (rescale=1.0/255. ) or (rescale=1./255)\n",
        "\n",
        "train_dir = '/tmp/rps/'\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, # Point the generator to the directory called Train that contains the sub-directories A or B.\n",
        "    target_size=(300,300), # This is ensuring that images are uniformly resized. Images are resized as they are loaded. No change to the source image.\n",
        "                            # If compact, then use (150,150) or whatever.\n",
        "    batch_size=20, # If there are 1000 images and batch size is 20, there will be 50 batches.\n",
        "    class_mode='categorical' ## This is different from binary classification.\n",
        ")\n",
        "\n",
        "validation_dir = '/tmp/rps-test-set/'\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir, # Point the generator to the directory called Train that contains the sub-directories A or B.\n",
        "    target_size=(300,300), # This is ensuring that images are uniformly resized. Images are resized as they are loaded. No change to the source image.\n",
        "                            # If compact, then use (150,150) or whatever.\n",
        "    batch_size=20, # If there are 1000 images and batch size is 20, there will be 50 batches.\n",
        "    class_mode='categorical' ## This is different from binary classification.\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT2T-IIh4ZSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 4. Data Preprocessing. Set up data generators that will read images from subdirectories and auto label with the subdirectory name.\n",
        "# Need to add a dimension to the data. Make (10000, 28, 28) into (10000, 28, 28, 1)\n",
        "training_images = np.expand_dims(training_images, axis=3)\n",
        "testing_images = np.expand_dims(testing_images, axis=3)\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255) # Another has it as (rescale=1.0/255. ) or (rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255) # Another has it as (rescale=1.0/255. ) or (rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow(training_images, training_labels, batch_size=32) ## This line is subsumed later.\n",
        "validation_generator = validation_datagen.flow(testing_images, testing_labels, batch_size=32) ## This line is subsumed later.\n",
        "\n",
        "# print(training_images.shape)\n",
        "# print(testing_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu-MD3PW-QK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(300,300,3)),\n",
        "                                    # 1st argument above is no. of convolutions. This number can be 16, 32, 64 or 128.\n",
        "                                    # relu means returning x when x>0, else return 0.\n",
        "                                    # The Conv2D filter is 3-by-3. Output is 2 pixels smaller on x and y.\n",
        "                                    # If the filter is 5-by-5, output is 4 pixels smaller on x and y.\n",
        "                                    # input_shape is the desired size of the image 300x300 in colour. \n",
        "                                    ## I can compact input_shape to (150,150,3) or other values.\n",
        "                                    ## For Alternative 4, input_shape=(28,28,1) for sign language dataset.\n",
        "                                    tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                    tf.keras.layers.Conv2D(16, (3,3), activation='relu'), # I can add more convolutions.\n",
        "                                    tf.keras.layers.MaxPooling2D(2,2), # I can add more pooling layers.\n",
        "                                    tf.keras.layers.Conv2D(32, (3,3), activation='relu'), # I can add more convolutions.\n",
        "                                    tf.keras.layers.MaxPooling2D(2,2), # I can add more pooling layers.\n",
        "                                    tf.keras.layers.Dropout(0.5), # This is the OPTIONAL dropout layer. This number can be 0.2 or 0.5 etc.\n",
        "                                    # Dropouts remove a random number of neurons in your neural network. Dropout is a regularization technique.\n",
        "                                    tf.keras.layers.Flatten(), # Flatten to feed into the Dense Neural Network.\n",
        "                                    # tf.keras.layers.Dropout(0.5), # This is the OPTIONAL dropout layer. This number can be 0.2 or 0.5 etc.\n",
        "                                    # Dropouts remove a random number of neurons in your neural network. Dropout is a regularization technique.\n",
        "                                    tf.keras.layers.Dense(128, activation='relu'), # This number can be 64, 128, 256, 512 or 1024.\n",
        "                                    # tf.keras.layers.Dropout(0.5), # This is the OPTIONAL dropout layer. This number can be 0.2 or 0.5 etc.\n",
        "                                    # Dropouts remove a random number of neurons in your neural network. Dropout is a regularization technique.\n",
        "                                    tf.keras.layers.Dense(1, activation='sigmoid')]) # Only 1 output neuron for binary classification problem.\n",
        "                                    # Output is a single scalar between 0 and 1 for binary classification problem.\n",
        "                                    # tf.keras.layers.Dense(26, activation='softmax') \n",
        "                                    ## This is for Alternatives 3 and 4 which has 3 and 26 categories respectively.\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjFpUU40vWIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatives 1, 1A, 2 and 2A.\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "model.compile(optimizer=RMSprop(lr=0.001), #RMSprop automatically adapts the learning rate tuning for me. This number can be 0.001 or 1e-4.\n",
        "              loss='binary_crossentropy', # Use this for binary classification problem.\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xPlZG-hvZ7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 3.\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "model.compile(optimizer=RMSprop(lr=0.001), #RMSprop automatically adapts the learning rate tuning for me. This number can be 0.001 or 1e-4.\n",
        "              # optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy', # Use this for multi-class classification problem.\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPRrmB7Y8JfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 4.\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # optimizer=tf.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCTa-iMIaZXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model when using transfer learning.\n",
        "x = layers.Flatten()(last_output)\n",
        "x = layers.Dense(128, activation='relu')(x) # This number can be 64, 128, 256, 512 or 1024.\n",
        "x = layers.Dropout(0.2)(x) # Dropping out 20% of the neurons. This number can be 0.2 or 0.5 etc.\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(pre_trained_model.input, x)\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "model.compile(optimizer=RMSprop(lr=0.001), #RMSprop automatically adapts the learning rate tuning for me. This number can be 0.001 or 1e-4.\n",
        "              loss='binary_crossentropy', # Use this for binary classification problem.\n",
        "              metrics=['accuracy'])\n",
        "# model.summary() # Output expected to be very large."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-oAueTxS3Rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative CALLBACKS based on accuracy.\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.6): # This number can be 0.99, 0.998 or 0.999. Can also use val_accuracy.\n",
        "      print(\"\\nReached 60% accuracy so cancelling training!\") # Need to change the print accordingly.\n",
        "      self.model.stop_training = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCj5u7zTS7o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative CALLBACKS based on loss.\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('loss')<0.4): # Can also use val_loss.\n",
        "      print(\"\\nReached less than 40% loss so cancelling training!\") # Need to change the print accordingly.\n",
        "      self.model.stop_training = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7ef1E4MISLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1. Without callbacks.\n",
        "history = model.fit(train_generator, steps_per_epoch=8, epochs=15, verbose=1) \n",
        "# Training batch size previously was 128 multiplied by steps_per_epoch to go through all the training images.\n",
        "# verbose 0, 1, 2 shows the least, most, in between information.\n",
        "# verbose 1 or 2 will show training loss, training accuracy, validation loss and validation accuracy. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ei1qUX5J6MQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatives 1A, 2, 2A and 3. Without callbacks.\n",
        "history = model.fit(train_generator, steps_per_epoch=8, epochs=15, verbose=1, validation_data=validation_generator, validation_steps=8)\n",
        "# Training batch size previously was 128 multiplied by steps_per_epoch to go through all the training images.\n",
        "# Validation batch size previously was 32 multiplied by validation_steps to go through all the validation images.\n",
        "# verbose 0, 1, 2 shows the least, most, in between information.\n",
        "# verbose 1 or 2 will show training loss, training accuracy, validation loss and validation accuracy. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlxWHZrL-Hs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 4. Without callbacks.\n",
        "history = model.fit(train_generator, steps_per_epoch=len(training_images)/32, epochs=15, validation_data=validation_generator, validation_steps=len(testing_images)/32)\n",
        "model.evaluate(testing_images, testing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odiDukKcRKEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 1 with callbacks.\n",
        "callbacks = myCallback()\n",
        "history = model.fit(train_generator, steps_per_epoch=8, epochs=15, verbose=1, callbacks=[callbacks]) \n",
        "# Training batch size previously was 128 multiplied by steps_per_epoch to go through all the training images.\n",
        "# verbose 0, 1, 2 shows the least, most, in between information.\n",
        "# verbose 1 or 2 will show training loss, training accuracy, validation loss and validation accuracy. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8ppHVIER2c0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatives 1A, 2, 2A and 3 with callbacks.\n",
        "callbacks = myCallback()\n",
        "history = model.fit(train_generator, steps_per_epoch=8, epochs=15, verbose=1, validation_data=validation_generator, validation_steps=8, callbacks=[callbacks])\n",
        "# Training batch size previously was 128 multiplied by steps_per_epoch to go through all the training images.\n",
        "# Validation batch size previously was 32 multiplied by validation_steps to go through all the validation images.\n",
        "# verbose 0, 1, 2 shows the least, most, in between information.\n",
        "# verbose 1 or 2 will show training loss, training accuracy, validation loss and validation accuracy. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf9VyPDp-W-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternative 4 with callbacks.\n",
        "callbacks = myCallback()\n",
        "history = model.fit(train_generator, steps_per_epoch=len(training_images)/32, epochs=15, validation_data=validation_generator, validation_steps=len(testing_images)/32, callbacks=[callbacks])\n",
        "model.evaluate(testing_images, testing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcAdr9HHwN_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra code. To save model.\n",
        "model.save(\"rps.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6QoVD9uK-oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra code. To upload my own files for the model to predict. I have not tidied up much of this code.\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300,300)) # The target size needs to match the input size of the model. \n",
        "                                                    # If compact, then use (150,150) or whatever.\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\") # Change the printing accordingly.\n",
        "  else:\n",
        "    print(fn + \" is a horse\") # Change the printing accordingly."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "024zTtVTLMfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra code. To visualise intermediate representations. I have not tidied up much of this code.\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Let's define a new Model that will take an image as input, and will output \n",
        "# intermediate representations for all layers in the previous model after the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "# visualization_model = Model(img_input, successive_outputs)\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random input image from the training set.\n",
        "a_img_files = [os.path.join(train_a_dir, fname) for fname in train_a_fnames]\n",
        "b_img_files = [os.path.join(train_b_dir, fname) for fname in train_b_fnames]\n",
        "img_path = random.choice(a_img_files + b_img_files)\n",
        "img = load_img(img_path, target_size=(300, 300)) # If compact, then use (150,150) or whatever. The shape below will change accordingly.\n",
        "x = img_to_array(img)  # Numpy array with shape (300, 300, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 300, 300, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# Now let's display our representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # We'll tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io7NC18dd_RA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc)) # Get the number of epochs.\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.legend(loc=0) # This is to place legend at bottom right of graph.\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend() \n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}